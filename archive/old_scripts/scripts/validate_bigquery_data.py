#!/usr/bin/env python3
"""
Validate TopGolf Data in BigQuery
Checks data quality, completeness, and structure after extraction
"""

import os
import sys
from datetime import datetime

from google.cloud import bigquery

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from paidsearchnav.core.config import Settings


def validate_data_structure():
    """Validate that all expected tables exist with proper structure"""
    print("üîç VALIDATING BIGQUERY DATA STRUCTURE")
    print("=" * 60)

    settings = Settings.from_env()
    if not settings.bigquery:
        print("‚ùå BigQuery not configured")
        return False

    client = bigquery.Client(project=settings.bigquery.project_id)
    dataset_id = f"{settings.bigquery.project_id}.{settings.bigquery.dataset_id}"

    expected_tables = [
        "search_terms_raw",
        "keywords_raw",
        "campaigns_raw",
        "device_performance_raw",
        "geo_performance_raw",
        "time_performance_raw",
    ]

    existing_tables = []

    try:
        dataset = client.get_dataset(dataset_id)
        tables = list(client.list_tables(dataset))
        existing_tables = [table.table_id for table in tables]

        print(f"üìä Dataset: {dataset_id}")
        print(f"üìÖ Created: {dataset.created}")
        print(f"üìà Total Tables: {len(existing_tables)}")
        print()

        for table_name in expected_tables:
            if table_name in existing_tables:
                print(f"‚úÖ {table_name} - EXISTS")
            else:
                print(f"‚ùå {table_name} - MISSING")

        print()
        print("üìã Additional Tables Found:")
        for table_name in existing_tables:
            if table_name not in expected_tables:
                print(f"   ‚Ä¢ {table_name}")

        return all(
            table in existing_tables for table in expected_tables[:3]
        )  # At least core tables

    except Exception as e:
        print(f"‚ùå Error accessing BigQuery: {e}")
        return False


def validate_data_quality():
    """Check data quality and completeness"""
    print("\nüîç VALIDATING DATA QUALITY")
    print("=" * 60)

    settings = Settings.from_env()
    client = bigquery.Client(project=settings.bigquery.project_id)
    dataset_id = f"{settings.bigquery.project_id}.{settings.bigquery.dataset_id}"

    # Data quality queries
    queries = {
        "search_terms_overview": f"""
            SELECT
                COUNT(*) as total_records,
                COUNT(DISTINCT search_term) as unique_search_terms,
                COUNT(DISTINCT campaign_id) as unique_campaigns,
                COUNT(DISTINCT ad_group_id) as unique_ad_groups,
                SUM(SAFE_CAST(cost AS FLOAT64)) as total_cost,
                SUM(SAFE_CAST(clicks AS INT64)) as total_clicks,
                SUM(SAFE_CAST(conversions AS FLOAT64)) as total_conversions,
                MIN(date) as earliest_date,
                MAX(date) as latest_date
            FROM `{dataset_id}.search_terms_raw`
        """,
        "keywords_overview": f"""
            SELECT
                COUNT(*) as total_records,
                COUNT(DISTINCT keyword) as unique_keywords,
                COUNT(DISTINCT campaign_id) as unique_campaigns,
                COUNT(DISTINCT match_type) as unique_match_types,
                SUM(SAFE_CAST(cost AS FLOAT64)) as total_cost,
                SUM(SAFE_CAST(clicks AS INT64)) as total_clicks,
                SUM(SAFE_CAST(conversions AS FLOAT64)) as total_conversions,
                AVG(SAFE_CAST(quality_score AS FLOAT64)) as avg_quality_score
            FROM `{dataset_id}.keywords_raw`
        """,
        "campaigns_overview": f"""
            SELECT
                COUNT(*) as total_records,
                COUNT(DISTINCT campaign_id) as unique_campaigns,
                COUNT(DISTINCT campaign_type) as unique_campaign_types,
                SUM(SAFE_CAST(cost AS FLOAT64)) as total_cost,
                SUM(SAFE_CAST(clicks AS INT64)) as total_clicks,
                SUM(SAFE_CAST(conversions AS FLOAT64)) as total_conversions,
                AVG(SAFE_CAST(efficiency_score AS FLOAT64)) as avg_efficiency_score
            FROM `{dataset_id}.campaigns_raw`
        """,
        "device_performance_overview": f"""
            SELECT
                device,
                COUNT(*) as records,
                SUM(SAFE_CAST(cost AS FLOAT64)) as cost,
                SUM(SAFE_CAST(clicks AS INT64)) as clicks,
                SUM(SAFE_CAST(conversions AS FLOAT64)) as conversions,
                ROUND(AVG(SAFE_CAST(conversion_rate AS FLOAT64)), 2) as avg_conv_rate
            FROM `{dataset_id}.device_performance_raw`
            GROUP BY device
            ORDER BY cost DESC
        """,
    }

    results = {}

    for query_name, query in queries.items():
        try:
            print(f"\nüìä {query_name.replace('_', ' ').title()}:")
            print("-" * 40)

            query_job = client.query(query)
            rows = list(query_job.result())

            if query_name == "device_performance_overview":
                for row in rows:
                    print(
                        f"   {row.device}: ${row.cost:,.2f} cost, {row.clicks:,} clicks, {row.conversions:.1f} conv ({row.avg_conv_rate}% rate)"
                    )
            else:
                for row in rows:
                    results[query_name] = dict(row)
                    for key, value in row.items():
                        if isinstance(value, float) and key.startswith(
                            ("total_cost", "avg_")
                        ):
                            print(f"   {key}: {value:,.2f}")
                        elif isinstance(value, int) and "total" in key:
                            print(f"   {key}: {value:,}")
                        else:
                            print(f"   {key}: {value}")

        except Exception as e:
            print(f"‚ùå Error in {query_name}: {e}")
            results[query_name] = {"error": str(e)}

    return results


def validate_data_consistency():
    """Check for data consistency issues"""
    print("\nüîç VALIDATING DATA CONSISTENCY")
    print("=" * 60)

    settings = Settings.from_env()
    client = bigquery.Client(project=settings.bigquery.project_id)
    dataset_id = f"{settings.bigquery.project_id}.{settings.bigquery.dataset_id}"

    consistency_queries = {
        "null_values_check": f"""
            SELECT
                'search_terms' as table_name,
                COUNTIF(search_term IS NULL) as null_search_terms,
                COUNTIF(campaign_id IS NULL) as null_campaign_ids,
                COUNTIF(cost IS NULL) as null_costs,
                COUNT(*) as total_records
            FROM `{dataset_id}.search_terms_raw`

            UNION ALL

            SELECT
                'keywords' as table_name,
                COUNTIF(keyword IS NULL) as null_keywords,
                COUNTIF(campaign_id IS NULL) as null_campaign_ids,
                COUNTIF(cost IS NULL) as null_costs,
                COUNT(*) as total_records
            FROM `{dataset_id}.keywords_raw`
        """,
        "data_ranges_check": f"""
            SELECT
                'search_terms' as table_name,
                MIN(SAFE_CAST(cost AS FLOAT64)) as min_cost,
                MAX(SAFE_CAST(cost AS FLOAT64)) as max_cost,
                MIN(SAFE_CAST(clicks AS INT64)) as min_clicks,
                MAX(SAFE_CAST(clicks AS INT64)) as max_clicks,
                MIN(SAFE_CAST(conversions AS FLOAT64)) as min_conversions,
                MAX(SAFE_CAST(conversions AS FLOAT64)) as max_conversions
            FROM `{dataset_id}.search_terms_raw`

            UNION ALL

            SELECT
                'keywords' as table_name,
                MIN(SAFE_CAST(cost AS FLOAT64)) as min_cost,
                MAX(SAFE_CAST(cost AS FLOAT64)) as max_cost,
                MIN(SAFE_CAST(clicks AS INT64)) as min_clicks,
                MAX(SAFE_CAST(clicks AS INT64)) as max_clicks,
                MIN(SAFE_CAST(conversions AS FLOAT64)) as min_conversions,
                MAX(SAFE_CAST(conversions AS FLOAT64)) as max_conversions
            FROM `{dataset_id}.keywords_raw`
        """,
    }

    for query_name, query in consistency_queries.items():
        try:
            print(f"\nüìä {query_name.replace('_', ' ').title()}:")
            print("-" * 40)

            query_job = client.query(query)
            rows = list(query_job.result())

            for row in rows:
                print(f"   {row.table_name}:")
                for key, value in row.items():
                    if key != "table_name":
                        if isinstance(value, float):
                            print(f"     {key}: {value:.2f}")
                        else:
                            print(f"     {key}: {value}")
                print()

        except Exception as e:
            print(f"‚ùå Error in {query_name}: {e}")


def generate_data_report():
    """Generate comprehensive data validation report"""
    print("\nüìã GENERATING DATA VALIDATION REPORT")
    print("=" * 60)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"/Users/robertwelborn/PycharmProjects/PaidSearchNav/topgolf_bigquery_validation_{timestamp}.md"

    with open(report_path, "w") as f:
        f.write("# TopGolf BigQuery Data Validation Report\n")
        f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("**Customer ID:** 577-746-1198\n\n")

        f.write("## Validation Summary\n")
        f.write("- ‚úÖ Data structure validation completed\n")
        f.write("- ‚úÖ Data quality checks completed\n")
        f.write("- ‚úÖ Data consistency validation completed\n\n")

        f.write("## Next Steps\n")
        f.write("1. Run all 21 analyzers against validated data\n")
        f.write("2. Test ML model training with real data\n")
        f.write("3. Validate prediction accuracy\n")
        f.write("4. Test API endpoints\n")
        f.write("5. Run end-to-end pipeline test\n")

    print(f"‚úÖ Report saved to: {report_path}")
    return report_path


def main():
    """Main validation function"""
    print("üöÄ TOPGOLF BIGQUERY DATA VALIDATION")
    print("=" * 60)
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # Step 1: Validate structure
    structure_valid = validate_data_structure()
    if not structure_valid:
        print("‚ùå Data structure validation failed")
        return False

    # Step 2: Validate quality
    quality_results = validate_data_quality()

    # Step 3: Validate consistency
    validate_data_consistency()

    # Step 4: Generate report
    report_path = generate_data_report()

    print("\n" + "=" * 60)
    print("‚úÖ BIGQUERY DATA VALIDATION COMPLETED")
    print("=" * 60)
    print("Ready to proceed with analyzer testing!")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
